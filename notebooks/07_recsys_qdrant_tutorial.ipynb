{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41f17569-20e2-402d-ae4e-a7a8d55295de",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Recommendation Systems, Vector Databases, and Music"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "903f83d2",
   "metadata": {},
   "source": [
    "![main](../images/main_pic.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "690196c5",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01ede68f",
   "metadata": {},
   "source": [
    "1. Overview\n",
    "2. The Challenge\n",
    "3. Audio Data\n",
    "    - Intro to Audio Data\n",
    "    - Data Preparation\n",
    "4. Vector Databases\n",
    "    - What are they?\n",
    "    - Why do we need them?\n",
    "    - How can we use them?\n",
    "    - Enter Qdrant\n",
    "        - Getting Started\n",
    "        - Adding Points\n",
    "        - Payloads\n",
    "        - Search\n",
    "5. Models and Vectors\n",
    "    - What are transformers?\n",
    "    - What are Embeddings?\n",
    "    - Fine tunning Wav2Vec\n",
    "    - Extracting Embeddings\n",
    "6. Putting it all together\n",
    "    - Basics of Recommender Systems\n",
    "    - Loading up Qdrant\n",
    "    - Building a UI\n",
    "7. Final Thoughts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c45dab1c",
   "metadata": {},
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d98a101",
   "metadata": {},
   "source": [
    "Vector databases are a \"relatively\" new way for interacting with abstract data representations derived from opaque machine learning models -- deep learning architectures being the most common ones. These representations are often called vectors or embeddings and they are a compressed version of the data used to train a machine learning model to accomplish a task (e.g., sentiment analysis, speech recognition, object detection, and many more).\n",
    "\n",
    "One of the best features of a vector database is their ability to serve as the building block of a recommendation system, and in this tutorial, you'll learn how to accomplish such a feast for a music use case. Before we go over what our recommendation system would look like, we'll start with a brief introduction on how to work with audio data, and we'll then move on to the main components of a vector database using [Qdrant](qdrant.tech).\n",
    "\n",
    "Qdrant \"is a vector similarity search engine that provides a production-ready service with a convenient API to store, search, and manage points (i.e. vectors) with an additional payload.\" You can get started with plain python using the `qdrant-client`, pull the latest docker image of `qdrant` and connect to it locally, or try out Qdrant's Cloud free tier option until you are ready to make the full switch.\n",
    "\n",
    "Now, a tutorial is most useful when it involves a real use case, so let's go ahead and describe ours."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d844a022",
   "metadata": {},
   "source": [
    "## 2. The Challenge"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6811a13f",
   "metadata": {},
   "source": [
    "Building recommendation systems can be quite challenging. For starters, we never know apriori the needs and wants of the new customer of a store or the new user of a mobile app, and this makes is difficult to recommend, say, a toaster brand to someone searching for skillets at the store, or a Beatles' song to new users that just listened to a bachata by Romeo Santos and Drake.\n",
    "\n",
    "The aforementioned problems -- \"new user\" == \"no data\" -- belong to the \"cold start\" family of problems in recommender systems and, while these problems might not go away anytime soon, there are ways to bypass them and serve relevant results from the get go, and one answer is vector databases. That said, that's what we will work on in this tutorial, we will build a music recommendation system on top of Qdrant -- an open-source vector database designed for flexibility, scalability and ease of use -- that serves relevant results from the get go."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "197a01f7",
   "metadata": {},
   "source": [
    "## 3. Audio Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2209f84e",
   "metadata": {},
   "source": [
    "The data we'll use can be downloaded from Kaggle [here](https://www.kaggle.com/datasets/jorgeruizdev/ludwig-music-dataset-moods-and-subgenres?resource=download&select=labels.json), and it consists of the following pieces.\n",
    "- `mp3/mp3`: two mp3 directories (unnecessarily) with audio files categorized by genre (e.g., latin, hip_hop, etc.). The name of each song is also the unique ID of the same. For simplicity make sure you move the files in the second mp3 directory up one level.\n",
    "- `spectogram`: Mel Frequency Spectograms of each song saved as `.npy` files.\n",
    "- `MFCCS`: This folder contains a .npy file with the extracted MFFCs of each song. Each `.npy` file contains around 10 MFFCs of 3s of duration.\n",
    "- `labels.json`: metadata about each song including genre, artist, song, etc. We can use the unique ID to merge it with the main dataset.\n",
    "- `subgenres.json`: metadata about the subgenre of each song. We can use the unique ID to merge it with the main dataset.\n",
    "\n",
    "Once you download the data and unzip it inside your data directory, make sure you have the directory arranged in the following way to match this tutorial.\n",
    "\n",
    "```sh\n",
    "../data/ludwig_music_data\n",
    "├── labels.json\n",
    "├── mfccs\n",
    "│   ├── blues\n",
    "│   ├── ...\n",
    "├── mp3\n",
    "│   ├── blues\n",
    "│   ├── classical\n",
    "│   ├── electronic\n",
    "│   ├── funk _ soul\n",
    "│   ├── hip hop\n",
    "│   ├── jazz\n",
    "│   ├── latin\n",
    "│   ├── pop\n",
    "│   ├── reggae\n",
    "│   └── rock\n",
    "├── spectogram\n",
    "│   └── spectogram\n",
    "└── subgeneres.json\n",
    "```\n",
    "\n",
    "Please note that you can substitute this (12gb) dataset for another of your choosing, provided it is for a classification task, and follow the tutorial with barely any tweaks to the code.\n",
    "\n",
    "Now that we know a bit about the challenge we will be tackling and the data we will be working with, let's talk about audio data in general to build an intuition on how things work."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9be7cf20",
   "metadata": {},
   "source": [
    "### 3.1 Intro to Audio Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29019bf2",
   "metadata": {},
   "source": [
    "Audio data refers to any type of sound that can be stored and transmitted in a digital format. This can include music, spoken words, and other types of sound recordings. In some ways, it's similar to how text can be stored and transmitted in a digital format. Hence, audio data is way of representing sound in a shape and form that can be processed and analyzed by computers. A wide range of applications use audio data and these include music production, telecommunications, and digital assistants like Siri and Alexa.\n",
    "\n",
    "For data science use cases, we need to convert sound data to Mel spectrograms for use in machine learning models, the audio data is first divided into small segments. Each segment is then processed using a mathematical operation known as the Fourier transform, which breaks down the sound into its component frequencies. \n",
    "\n",
    "After the Fourier transform has been applied, mel-scale filter banks are used to group the frequencies into a set of bands that more closely match the human auditory system's perception of sound. These filter banks essentially amplify some frequency ranges while reducing others. This process results in a set of values for each segment of audio, which can be arranged to form a spectrogram.\n",
    "\n",
    "Mel-spectrograms are useful in machine learning applications as they provide a way to represent audio data in a format that can be easily processed and analyzed by computer models. The mel-spectrograms can be used as inputs to machine learning models, which can then learn to recognize patterns and classify different types of sounds.\n",
    "\n",
    "Now that we know a little bit about audio data, let's examine a sample to get an intuition for how the process works.\n",
    "\n",
    "Before you run any line of code, make sure you create a virtual environment with the following packages.\n",
    "\n",
    "```bash\n",
    "# with conda\n",
    "mamba env create -n my_env python=3.10\n",
    "mamba activate my_env\n",
    "\n",
    "# or with virtualenv\n",
    "python -m venv venv\n",
    "source venv/bin/activate\n",
    "\n",
    "# install packages\n",
    "pip install qdrant-client transformers datasets pandas numpy streamlit\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd6c186-51ef-4dcd-9007-217cad90678c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio as player\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b1b956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a35362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08a0117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0be6e1f5",
   "metadata": {},
   "source": [
    "### 3.2 Data Prep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0e175c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52253950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2d3a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_data = load_dataset(\n",
    "    \"audiofolder\", data_dir=\"../data/ludwig_music_data/mp3/\", split=\"train\"\n",
    ").shuffle(seed=42).select(range(200))\n",
    "\n",
    "music_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b449e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abea800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_the_id(data):\n",
    "    data['audio_path'] = data['audio']['path']\n",
    "    data['idx'] = data['audio_path'].split(\"/\")[-1].replace(\".mp3\", '')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6e7190",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_data = music_data.map(get_the_id, num_proc=6)\n",
    "music_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4548f579",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_data.unique(\"label\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66dde457",
   "metadata": {},
   "source": [
    "Time for the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8caf96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9599705",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_json(\"../data/ludwig_music_data/labels.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8980dbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "labels['tracks'].iloc[choice(range(200))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c71293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(x):\n",
    "    try:\n",
    "        artist = list(x['artist'].values())[0]\n",
    "        genre = list(x['genre'].values())[0]\n",
    "        name = list(x['name'].values())[0]\n",
    "    except:\n",
    "        artist = \"Unknown\"\n",
    "        genre = \"Unknown\"\n",
    "        name = \"Unknown\"\n",
    "    return pd.Series([artist, genre, name], index=['artist', 'genre', 'name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953f059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_labels = labels['tracks'].apply(get_metadata).reset_index()\n",
    "clean_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18bd264",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_labels.name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da269c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_data = music_data.to_pandas().merge(\n",
    "    right=clean_labels, left_on='idx', right_on='index', how=\"left\"\n",
    ").drop(\"index\", axis=1)\n",
    "\n",
    "music_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b398f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31e649f",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_data = Dataset.from_pandas(music_data, preserve_index=False)\n",
    "music_data = music_data.cast_column('audio', Audio(sampling_rate=16_000))\n",
    "music_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c5a6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_data[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eaf41a1e-44eb-48c1-881c-ce5d865487eb",
   "metadata": {},
   "source": [
    "## 4. Vector Databases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24067f6f",
   "metadata": {},
   "source": [
    "### 4.1 What Are They?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0ee9c6a",
   "metadata": {},
   "source": [
    "A vector database is a type of database designed to store and query high-dimensional vectors efficiently. In traditional databases, data is typically organized in rows and columns, and queries are performed based on the values in those columns. However, in certain applications, such as machine learning, image recognition, natural language processing, and recommendation systems, data is often represented as vectors in a high-dimensional space.\n",
    "\n",
    "A vector in this context is a mathematical representation of an object or data point, where each element of the vector corresponds to a specific feature or attribute of the object. For example, in an image recognition system, a vector could represent an image, with each element of the vector representing a pixel value or a descriptor of that pixel.\n",
    "\n",
    "Vector databases are optimized for storing and querying these high-dimensional vectors efficiently, often using specialized data structures and indexing techniques. They enable fast similarity searches, allowing users to find vectors that are similar to a given query vector based on some distance metric, such as Euclidean distance or cosine similarity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa11fe96",
   "metadata": {},
   "source": [
    "### 4.2 Why do we need them?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aeded65e",
   "metadata": {},
   "source": [
    "Vector databases play a crucial role in various applications that require similarity search, such as recommendation systems, content-based image retrieval, and personalized search. By leveraging efficient indexing and search techniques, vector databases enable faster and more accurate retrieval of similar vectors, enabling advanced data analysis and decision-making."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d656cee3",
   "metadata": {},
   "source": [
    "### 4.3 How can we use them?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d11c574",
   "metadata": {},
   "source": [
    "To get started using vector databases, it is important that we understand the basics of how vectors are generated. We do not need to be a data scientist or a machine learning engineer, but some familiarity with machine learning (and/or the ability to follow a comprehensive tutorial) would be enough to help us get our hands on some vectors. \n",
    "\n",
    "Once we train an algorithm, most-likely a deep learning one, you will have at your disposal the vector representation of the data used to train the model, and the model itself to transform new data into vectors. Note that, even if you trained your algorithm for a particular task, what you need is the structured representation of your unstructured data (e.g., images, audio, text, etc.) rather than the predictive function itself. The goal is to compare the new sample to existing ones to get the most similar results on the fly.\n",
    "\n",
    "The next step is to pick a vector database that aligns with what you, your team, or company is trying to do (e.g., recommendation system, semantic search, ranking, etc.). There are many solutions out and they all differ in terms of ease of use (i.e., how many more tools you need in order to get started), distribution (e.g. SaaS vs Open Source), accessibility (i.e., can you find new creative ways to use the low-level parts of the database?), and coverage (e.g., similarity metrics available and capabilities such as payloads).\n",
    "\n",
    "Once you have you vector database set up, and have gotten familiarized with the API, start adding vector and create a simple UI to test your use case, for example, use streamlit or nicegui to put together a nice looking prototype that you can share and get feedback from. If everyone is happy with the results, start drafting a plan for how you will productionize, maintain, and monitor your database, as well as what are the steps to provide access to the database to developers or other data professionals working on recommendation systems.\n",
    "\n",
    "With that out of the way, let's get started with vector databases using the fastest growing open source tool in the market, Qdrant! 😎"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4796e65d",
   "metadata": {},
   "source": [
    "### 4.3 Enter Qdrant"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a24540ff",
   "metadata": {},
   "source": [
    "![qdrant](../images/qdrant_overview.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "373e33df",
   "metadata": {},
   "source": [
    "#### 4.3.1 Getting Started"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "115c0e96",
   "metadata": {},
   "source": [
    "In order to get started with Qdrant, we can either pull the latest docker image using `docker pull qdrant/qdrant`, and then run it with,\n",
    "```bash\n",
    "docker run -p 6333:6333 \\\n",
    "    -v $(pwd)/qdrant_storage:/qdrant/storage \\\n",
    "    qdrant/qdrant\n",
    "```\n",
    "or we can use its Python client and use in in-memory functionality to play around with it. We will opt for the former in this tutorial but feel free to use the in-memory version if you prefer.\n",
    "\n",
    "We'll start by instantiating our client using `host=\"localhost\"` and `port=6333` as opposed to `host=\":memory:\"` for the in-memory option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca10c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d425601",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "client"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "769875dd",
   "metadata": {},
   "source": [
    "In more OLTP or OLAP databases we call specific logical bundles of data `Tables`, but in vector databases, we refer to these collections of vectors as `collections`. In the same way in which we can create many tables in a database, we can create many collections in a vector-based db using a client. The key difference to note is that when we create a collection, we need to specify the width of the collection with the parameter `size=...`, and the similarity metric `distance=...` (which can be changed later on).\n",
    "\n",
    "Let's create our first collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bf5188",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_collection = client.create_collection(\n",
    "    collection_name=\"my_first_collection\",\n",
    "    vectors_config=models.VectorParams(size=100, distance=models.Distance.COSINE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f225c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can check that our collection was indeed created with\n",
    "client.get_collections()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a09f4fe",
   "metadata": {},
   "source": [
    "There's a couple of things to notice from what we have done.\n",
    "- The first is that when we initiated our docker image, we created a local directory, `qdrant_storage`, where all of our collections, plus their metadata, will be saved at. You can have a look at that directory in a *nix system with `tree qdrant_storage -L 2`. You should see the following.\n",
    "    ```bash\n",
    "    qdrant_storage\n",
    "    ├── aliases\n",
    "    │   └── data.json\n",
    "    ├── collections\n",
    "    │   └── my_first_collection\n",
    "    └── raft_state\n",
    "    ```\n",
    "- The second is that we used `client.create_collection` and this command can only be used once per collection. To recreate the collection with new parameters and the like, we would use `client.recreate_collection` instead.\n",
    "\n",
    "Now that we know how to create collections, let's create a bit of fake data and add some vectors to our collection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2a67064",
   "metadata": {},
   "source": [
    "#### 4.3.2 Adding Points"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be7a63c5",
   "metadata": {},
   "source": [
    "The points are the central entity that Qdrant operates with, and these points contain records consisting of a vector, an optional id and an optional payload (which we'll talk more about in the next section).\n",
    "\n",
    "The optional id can be represented by unassigned integers or UUIDs. We are going a range of numbers for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a13afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.uniform(low=-1.0, high=1.0, size=(1_000, 100))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cdc61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = list(range(len(data)))\n",
    "index[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ca3ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.upsert(\n",
    "    collection_name=\"my_first_collection\",\n",
    "    points=models.Batch(\n",
    "        ids=index,\n",
    "        vectors=data.tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d13ac820",
   "metadata": {},
   "source": [
    "We can retrieve specific points based on their ID (for example, artist X with ID 1000) and get some additional information from that result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28261210",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.retrieve(\n",
    "    collection_name=\"my_first_collection\",\n",
    "    ids=[100],\n",
    "    with_vectors=True # we can turn this on and off depending on our needs\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c735d63b",
   "metadata": {},
   "source": [
    "We can also update our collection one point at a time, for example, as new data comes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_song():\n",
    "    return np.random.uniform(low=-1.0, high=1.0, size=100).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.upsert(\n",
    "    collection_name=\"my_first_collection\",\n",
    "    points=[\n",
    "        models.PointStruct(\n",
    "            id=1000,\n",
    "            vector=create_song(),\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a57ae0ef",
   "metadata": {},
   "source": [
    "We can also delete it in a straightforward fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce47379",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.count(\n",
    "    collection_name=\"my_first_collection\", \n",
    "    exact=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54487be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete(\n",
    "    collection_name=\"my_first_collection\",\n",
    "    points_selector=models.PointIdsList(\n",
    "        points=[1000],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845f8a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.count(\n",
    "    collection_name=\"my_first_collection\", \n",
    "    exact=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1859dc4",
   "metadata": {},
   "source": [
    "#### 4.3.3 Payloads"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10c1b0e6",
   "metadata": {},
   "source": [
    "Qdrant has incredible features on top of speed and reliability, and one of its most useful ones is without a doubt the ability to store additional information along with vectors. In Qdrant terminology, this information is considered a payload and it is represented as a JSON file. In addition, not only can you get this information back when you search in the database, but you can also filter your search by the parameters in the payload, and we'll see how in a second.\n",
    "\n",
    "Imagine the fake vectors we created actually represented a song. If we were building a recommender system for songs then, naturally, the things we would want to get back would be the song itself, the artist, maybe the genre, and so on.\n",
    "\n",
    "What we'll do here is to take advantage of a Python package call `faker` and create a bit of information to add to our payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd1b168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcd03cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_something = Faker()\n",
    "fake_something.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2641810",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    payload.append(\n",
    "        {\n",
    "            \"artist\": fake_something.name(),\n",
    "            \"song\": \" \".join(fake_something.words()),\n",
    "            \"url_song\": fake_something.url(),\n",
    "            \"year\": fake_something.year(),\n",
    "            \"country\": fake_something.country()\n",
    "        }\n",
    "    )\n",
    "\n",
    "payload[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b74041",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.upsert(\n",
    "    collection_name=\"my_first_collection\",\n",
    "    points=models.Batch(\n",
    "        ids=index,\n",
    "        vectors=data.tolist(),\n",
    "        payloads=payload\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f614f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "resutls = client.retrieve(\n",
    "    collection_name=\"my_first_collection\",\n",
    "    ids=[10, 50, 100, 500],\n",
    "    with_vectors=False\n",
    ")\n",
    "resutls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28179b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "resutls[0].payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6507b72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.clear_payload(\n",
    "#     collection_name=\"my_first_collection\",\n",
    "#     points_selector=models.PointIdsList(\n",
    "#         points=index,\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f48bcd6",
   "metadata": {},
   "source": [
    "#### 4.3.4 Search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c58d28f3",
   "metadata": {},
   "source": [
    "Now that we have our vectors with an ID and a payload, we can explore a few of ways in which we can search for content when, in our use case, new music gets selected. Let's check it out.\n",
    "\n",
    "Say, for example, that a new song comes in and our model immediately transforms it into a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b714389",
   "metadata": {},
   "outputs": [],
   "source": [
    "living_la_vida_loca = create_song()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7902a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.search(\n",
    "    collection_name=\"my_first_collection\",\n",
    "    query_vector=living_la_vida_loca,\n",
    "    limit=10\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85407748",
   "metadata": {},
   "source": [
    "Now imagine that we only want Australian songs recommended to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59206e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "aussie_songs = models.Filter(\n",
    "    must=[models.FieldCondition(key=\"country\", match=models.MatchValue(value=\"Australia\"))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66355adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.search(\n",
    "    collection_name=\"my_first_collection\",\n",
    "    query_vector=living_la_vida_loca,\n",
    "    query_filter=aussie_songs,\n",
    "    limit=5\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6fd9e6f8",
   "metadata": {},
   "source": [
    "Lastly, say we want aussie songs but we don't care how new or old these songs are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54008c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.search(\n",
    "    collection_name=\"my_first_collection\",\n",
    "    query_vector=living_la_vida_loca,\n",
    "    query_filter=aussie_songs,\n",
    "    with_payload=models.PayloadSelectorExclude(exclude=[\"year\"]),\n",
    "    limit=5\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4692229",
   "metadata": {},
   "source": [
    "As you can see, the possibilities are endless."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4299ff02",
   "metadata": {},
   "source": [
    "## 5. Transformers and Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c39590fc",
   "metadata": {},
   "source": [
    "In the context of audio data, embeddings and transformers are used to process the sound waves and extract features that are useful for training machine learning models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58320005",
   "metadata": {},
   "source": [
    "### 5.1 What are transformers?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28b61764",
   "metadata": {},
   "source": [
    "Transformers are a type of neural network used for natural language processing, but they can also be used for processing audio data by breaking the sound waves into smaller parts and learning how those parts fit together to form meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f9e4b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "932d5d58",
   "metadata": {},
   "source": [
    "### 5.2 What are Embeddings?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f64dc0f",
   "metadata": {},
   "source": [
    "Embeddings are a way of representing audio data as vectors or numbers, which makes it easier for machine learning algorithms to process and analyze them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4c59ca9",
   "metadata": {},
   "source": [
    "### 5.3 Fine tunning Wav2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbc1463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from datasets import Audio\n",
    "from transformers import AutoModel, AutoFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20244511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio as player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c9ad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = music_data[0]['audio']['array']\n",
    "player(sample, rate=16_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f5cac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = music_data.unique(\"genre\")\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    "\n",
    "num_labels = len(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7de2090",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed2a1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "inputs = feature_extractor(\n",
    "    sample, sampling_rate=feature_extractor.sampling_rate, \n",
    "    return_tensors=\"pt\", padding=True, return_attention_mask=True\n",
    ").to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fe970d1",
   "metadata": {},
   "source": [
    "To download and instantiate a pre-trained model, we will use the AutoModel class from transformers.\n",
    "\n",
    "Note that there are other ways of doing the same.\n",
    "```python\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdf3c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\n",
    "    \"facebook/wav2vec2-base\", num_labels=num_labels, label2id=label2id, id2label=id2label\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a531e639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fbc3fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8bec30a",
   "metadata": {},
   "source": [
    "### 5.4 Extracting Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9504730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    last_hidden_state = model(**inputs.to(device)).last_hidden_state[:, 0]\n",
    "last_hidden_state.size(), last_hidden_state[0, :20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c44783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    return feature_extractor(\n",
    "        audio_arrays, sampling_rate=feature_extractor.sampling_rate, return_tensors=\"pt\",\n",
    "        max_length=16000, truncation=True, padding=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06a8ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_data = music_data.map(get_features, batched=True, batch_size=50)\n",
    "music_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3406baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_data.set_format(\"torch\", columns=[\"genre\", \"input_values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34497130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states(batch):\n",
    "    inputs = {k: v.to(device) for k, v in batch.items() if k in feature_extractor.model_input_names}\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state\n",
    "    return {\"hidden_state\": last_hidden_state[:, 0].cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0044ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_state = music_data.map(extract_hidden_states, batched=True, batch_size=50)\n",
    "hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ef1ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(hidden_state[\"hidden_state\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54bc868",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(hidden_state[\"hidden_state\"])[:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9468e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\n",
    "    '../data/recsys_1/vectors.npy', \n",
    "    np.array(hidden_state[\"hidden_state\"]), \n",
    "    allow_pickle=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "baece8b0",
   "metadata": {},
   "source": [
    "Embeddings and transformers are tools used to extract important information from audio data and make it easier for computers to understand and work with that information. They are used in a wide range of applications, from speech recognition to music analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51de40ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_state.select_columns(\n",
    "    [\"genre\", \"artist\", 'name', 'audio_path']\n",
    ").to_pandas().to_json(\"../data/recsys_1/payload.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8904f6c",
   "metadata": {},
   "source": [
    "## 6. Putting it All Together"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a69b7fbd",
   "metadata": {},
   "source": [
    "If you are using a notebook, this would be a good time to restart it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71571f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torchaudio, torch\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoFeatureExtractor\n",
    "from IPython.display import Audio as player"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5fc02b88",
   "metadata": {},
   "source": [
    "### 6.1 Basics of Recommender Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff4a899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5412a67",
   "metadata": {},
   "source": [
    "### 6.2 Loading Up Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e78b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = pd.read_json(\"../data/recsys_1/payload.json\", orient=\"records\").to_dict(orient=\"records\")\n",
    "payload[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a325463-805c-40dd-8eae-b5b6cf21e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.load(\"../data/recsys_1/vectors.npy\")\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572d295f-b837-498e-a2c3-806dd3faab6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = QdrantClient(\"localhost\", port=6333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41210967-3ec3-49b2-abd0-e7280eef617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.recreate_collection(\n",
    "    collection_name=\"music_recsys\",\n",
    "    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566ee497-edae-467d-8e6f-984de17f40e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_info = client.get_collection(collection_name=\"music_recsys\")\n",
    "collection_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49169591-ef42-4379-a63b-dfd5b8a6961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.http.models import CollectionStatus\n",
    "\n",
    "assert collection_info.status == CollectionStatus.GREEN\n",
    "assert collection_info.vectors_count == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe184c5-9858-4cef-9223-ed5540f92702",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.upsert(\n",
    "    collection_name=\"music_recsys\",\n",
    "    points=models.Batch(\n",
    "        ids=list(range(len(vectors))),\n",
    "        vectors=vectors.tolist(),\n",
    "        payloads=payload\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea2c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample, sampling_rate = torchaudio.load(\"../data/ludwig_music_data/mp3/latin/0A6nzrnr0lUreHRTx3fRru.mp3\")\n",
    "new_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57699893",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "model = AutoModel.from_pretrained(\"facebook/wav2vec2-base\")#.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baf4a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input = feature_extractor(\n",
    "    new_sample[0], sampling_rate=feature_extractor.sampling_rate, return_tensors=\"pt\", padding=True\n",
    ")#.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd75333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    vectr = model(**new_input).last_hidden_state[:, 0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66d5fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectr = vectr[0,].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05aac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "player(new_sample[1], rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1854a84-fa12-44bf-bbf6-a1939f822b01",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "results2 = client.search(\n",
    "    collection_name=\"music_recsys\",\n",
    "    query_vector=vectr.tolist(),\n",
    "    limit=10, \n",
    "    # with_vectors=True\n",
    ")\n",
    "results2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40521d74-0175-4347-94ae-bff972ed1bf0",
   "metadata": {},
   "source": [
    "### 6.3 Building a UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4b3745-1011-4a6a-b9a2-aae128c4ea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile recsys_app.py\n",
    "\n",
    "from transformers import AutoModel, AutoFeatureExtractor\n",
    "from qdrant_client import QdrantClient\n",
    "import torch, torchaudio\n",
    "import streamlit as st\n",
    "\n",
    "st.title(\"Music Recommendation App\")\n",
    "st.markdown(\"Upload your favorite songs and get a list of recommendations from our database of music.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModel.from_pretrained('models/my_model').to(device)\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"models/my_model\")\n",
    "client = QdrantClient(\"localhost\", port=6333)\n",
    "\n",
    "music_file = st.file_uploader(label=\"📀 Music file 🎸\",)\n",
    "\n",
    "if music_file:\n",
    "    st.audio(music_file)\n",
    "\n",
    "    waveform, sample_rate = torchaudio.load(music_file)\n",
    "    inputs = feature_extractor(\n",
    "        waveform[0], sampling_rate=feature_extractor.sampling_rate, \n",
    "        return_tensors=\"pt\", max_length=16000, truncation=True\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state[:, 0]\n",
    "    vectr = last_hidden_state.cpu().numpy()[0, :]\n",
    "\n",
    "    st.markdown(\"## Real Recommendations\")\n",
    "    results = client.search(collection_name=\"music_recsys\", query_vector=vectr, limit=4)\n",
    "    col1, col2 = st.columns(2)\n",
    "\n",
    "    with col1:\n",
    "        st.header(f\"Genre: {results[0].payload['genre']}\")\n",
    "        st.subheader(f\"Artist: {results[0].payload['artist']}\")\n",
    "        # st.audio(results[0].payload[\"audio_path\"])\n",
    "        \n",
    "        st.header(f\"Genre: {results[1].payload['genre']}\")\n",
    "        st.subheader(f\"Artist: {results[1].payload['artist']}\")\n",
    "        # st.audio(results[1].payload[\"audio_path\"])\n",
    "\n",
    "    with col2:\n",
    "        st.header(f\"Genre: {results[2].payload['genre']}\")\n",
    "        st.subheader(f\"Artist: {results[2].payload['artist']}\")\n",
    "        # st.audio(results[5].payload[\"audio_path\"])\n",
    "        \n",
    "        st.header(f\"Genre: {results[3].payload['genre']}\")\n",
    "        st.subheader(f\"Artist: {results[3].payload['artist']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b90973",
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run recsys_app.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19ba5243-7902-49a9-b0c0-04670eef4d3a",
   "metadata": {},
   "source": [
    "## 7. Final Thoughts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3c2f33b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
