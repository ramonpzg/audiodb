# audiodb

[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/ramonpzg/audiodb/HEAD)

## Table of Contents

1. Overview
2. The Data
3. The Tools
4. Setup
5. Next Steps

## 1. Overview

The goal of this project is build a music recommender system using tunes created 
from a diffusion model while contrasting these same tunes to recommendations 
from real songs from different artists. The key tool enabling this recommender system is 
[Qdrant](https://qdrant.tech/), "a vector similarity search engine that provides a 
production-ready service with a convenient API to store, search, and manage points
- vectors with an additional payload."

The task of the transformer is to classify the genre of 

The diffusion model

Once you have the environment set up and the unzipped dataset directory, `Audio`, 
inside a `data` directory, you will be able reproduced with the following commands.

```sh
## 1
mkdir models data data/hidden_state data/processed data/payloads data/external

## 2
`docker pull qdrant/qdrant`

## 3
`dvc repro`
```

## 2. The Data

The dataset can be found on [Kaggle](https://www.kaggle.com/datasets/carlossalazar65/tropical-genres-dataset), 
and you will need to have an account to download it. Since music is subject to many copyrights and other 
legalities, it contains no information other than many ~30-second clips of songs from genres 
such as Bachata, Merengue, Salsa, Cumbia, and Vallenato. The dataset is particularly appealing 
to me as the first two genres, Bachata and Merengue, were both born in my home country, 
the Dominican Republic.

A few things to note:
- Since we do not know the artist or the name of the song (and because song recognition APIs
are actually very difficult to find or use), we will use Fake names generated by `faker` 
to fill in the artist box and we will leave the name of the song in blank for now.
- The dataset does not contain an index so we will create one of 5 digits to help with the design of this project.


## 3. The Tools

- `ðŸ¤— transformers`
- `ðŸ¤— datasets`
- `pandas`
- `NumPy`
- `PyTorch`
- `Faker`
- `Qdrant`


## 4. Setup

First, make sure you have [docker](https://docs.docker.com/get-docker/) installed as this is a prerequisite to use Qdrant.

Next, you get started using Qdrant by running the following commands.

```sh
## 1
docker pull qdrant/qdrant

## 2
docker run -p 6333:6333 \
    -v $(pwd)/qdrant_storage:/qdrant/storage \
    qdrant/qdrant
```

Please note, you will need to run the first command above before we get started, but the second is not necessary. 

### Conda Users

#### First Step

Open up your terminal and navigate to a directory of your choosing in your computer. 
Once there, run the following command to get the code for the session.

```sh
 git clone git@github.com:ramonpzg/audiodb.git
```

Conversely, you can click on the green `download` button at the top and donwload all 
files to your desired folder/directory. Once you download it, unzip it and move on to 
the second step.

#### Second Step

To get all dependencies, packages and everything else that would be useful to reproduce 
this project, you can recreate the environment by first going into the directory for the project.

```sh
cd audiodb
```

Then you will need to create an environment with all of the dependancies needed 
for the session by running the following command.

```sh
conda create -n vdb_audio python=3.10
conda activate vdb_audio
conda install --yes --file requirements.txt
# OR
pip install -f requirements.txt


## Conversely

python -m venv venv

python -m venv venv
source venv/bin/activate
pip install -f requirements.txt
```

#### Third Step

Open up VSCode or Jupyter Lab and you should be ready to go.

```sh
jupyter lab

# or

code .
```

## Running the App

If you'd like to see a nicer version (with memes) of the answers for this challenge. Run the following command in your terminal and inside the environment for this project.

```sh
streamlit run src/app.py
```

A new window should pop up in no time. Otherwise, you can go to http://localhost:8501 and view the app there.



## 5. Next Steps

- [ ] Fine-tune diffuser model
- [ ] Get actual artists for each song
- [ ] Add more songs and artists
- [ ] Create a better pipeline with MetaFlow
- [ ] Add explainability step
- [ ] Add DB for queries used and outputs
- [ ] Add human-in-the-loop feedback mechanism for data flywheel